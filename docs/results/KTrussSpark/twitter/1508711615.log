/home/i/spark-2.2/bin/spark-submit --class ir.ac.sbu.graph.ktruss.spark.KTrussSpark --total-executor-cores 120 --master spark://alemi-1:7077 /home/i/subgraph-mining/target/subgraph-mining-1.0-jar-with-dependencies.jar hdfs://alemi-1/graph-data/twitter 700 5
[SBM] Input: hdfs://alemi-1/graph-data/twitter, partitionNum: 700, k: 5
[SBM] Spark conf: spark.app.name=KTrussSpark-700-twitter
spark.cores.max=120
spark.driver.maxResultSize=1g
spark.driver.memory=2g
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.maxExecutors=50
spark.dynamicAllocation.minExecutors=20
spark.eventLog.dir=hdfs://alemi-1/shared/spark-logs
spark.eventLog.enabled=true
spark.executor.cores=4
spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:-ResizePLAB -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:+PrintFlagsFinal -XX:+UseCompressedOops -verbose:gc -XX:InitiatingHeapOccupancyPercent=20
spark.executor.memory=20G
spark.history.fs.logDirectory=hdfs://alemi-1/shared/spark-logs
spark.io.compression.codec=lz4
spark.io.compression.lz4.blockSize=32k
spark.jars=file:/home/i/subgraph-mining/target/subgraph-mining-1.0-jar-with-dependencies.jar
spark.kryo.classesToRegister=
spark.kryo.unsafe=true
spark.kryoserializer.buffer=256k
spark.kryoserializer.buffer.max=256m
spark.local.dir=/mnt/sde/spark-data,/mnt/sdf/spark-data
spark.master=spark://alemi-1:7077
spark.rdd.compress=false
spark.reducer.maxSizeInFlight=32m
spark.rpc.netty.dispatcher.numThreads=3
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.shuffle.compress=true
spark.shuffle.file.buffer=32k
spark.shuffle.io.numConnectionsPerPeer=3
spark.shuffle.io.preferDirectBufs=false
spark.shuffle.service.enabled=true
spark.submit.deployMode=client
spark.worker.cleanup.enabled=true
